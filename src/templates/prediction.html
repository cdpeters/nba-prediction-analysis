{% extends 'navbar.html' %}

<!--------------------------------- HEAD -------------------------------------->
{% block head %}
<title>PREDICTION</title>
<script type="module" src="https://public.tableau.com/javascripts/api/tableau.embedding.3.latest.min.js"></script>
<!-- <link rel="stylesheet" href="style.css"> -->
{% endblock head %}

<!--------------------------------- BODY -------------------------------------->
{% block body %}
<div
  class="bg-image"
  style="
   width: 100%;
   height: 250px;
   background: url('https://ftw.usatoday.com/wp-content/uploads/sites/90/2016/10/usatsi_9349706-copy.jpg?w=1000&h=600&crop=1');
   background-size: cover;
   background-position: 100% 20%;
   opacity:0.9;">
</div>
<div class="px-5 pt-5 pb-3 mx-5">
  <div class="pb-2">
    <h2 class="pb-1 border-bottom text-white">Machine Learning Model: Logistic Regression</h2>
  </div>
  <br>
  <div class="d-flex flex-column align-items-center">
    <h4 class="align-self-start text-white">Formulation</h4>
    <p class="text-white w-100">
      The goal of predicting an NBA champion can be formulated as a binary
      classification problem where the two classes are <strong>champion</strong>
      and <strong>non-champion</strong>. Using historical NBA data, we know both
      classes exactly for all past seasons and thus the use of a supervised
      learning model is a fitting approach for our problem. Because we have
      formulated champion prediction as a binary classification problem, we
      chose logistic regression as the appropriate model for the job.
    </p>
    <p class="text-white w-100">
      The data collected during the web scraping process included both regular
      season and playoff data. Due to the fact that playoff data involves a
      smaller number of games per team and, crucially, an unequal number of
      games played per team (based on how far a team goes in the elimination
      tournament), it was decided to only use regular season data as the input
      for our model.
    </p>
  </div>
  <br>
  <div class="d-flex flex-column align-items-center">
    <h4 class="align-self-start text-white">Pre-Processing and Exploratory Analysis</h4>
    <p class="text-white w-100">
      We began pre-processing by filtering out the non-playoff teams and
      selecting the range of seasons identified in previous sections as
      representative of the style of the modern game (2016-2021). After a few
      transformations, all three stats tables (traditional, advanced, and
      miscellaneous) were merged together into one table containing our complete
      set of stats ready for exploratory analysis. Note that clutch stats were
      excluded from our dataset for the same reasons that playoff stats were
      excluded (low and unequal number of games per team).
    </p>
    <p class="text-white w-100">
      To begin exploring the data, we investigated the relationships between the
      stats to see the degree of linear dependence in our dataset. The chart
      below shows a Pearson correlation coefficient heatmap for our dataset:
    </p>
    <figure class="d-flex flex-column align-items-center">
      <img src="{{ url_for('static', filename='images/machine_learning/correlation_heatmap_all_features.png') }}"
            alt="correlation heatmap for all stats"
          style="width: 854px;" />
      <figcaption class="pt-1 text-white">Fig 1. Correlation coefficient heatmap for all stats</figcaption>
    </figure>
    <br>
    <p class="text-white w-100">
      The heat map reveals several statistics that have strong dependence on one
      or more of the other statistics. As an example, looking at the horizontal
      rows for effective field goal percentage (EFG%) and true shooting
      percentage (TS%), the correlation coefficient between them is high and
      thus their entire row of correlations look very similar. It is important
      to exclude one of these stats in pairings like this when moving forward,
      as they essentially represent the same information.
    </p>
    <p class="text-white w-100">
      Below are two charts that take a closer look at maximum and 80th
      percentile values of correlation coefficient:
    </p>
    <figure class="d-flex flex-column align-items-center">
      <img src="{{ url_for('static', filename='images/machine_learning/max_correlation_coefficients.png') }}"
           alt="max correlation coefficients"
         style="width: 654px;" />
      <figcaption class="pt-1 text-white">Fig 2. Stats having a maximum correlation coefficient < 0.8</figcaption>
    </figure>
    <br>
    <p class="text-white w-100">
      These are the stats with maximum correlations under 0.8. This was used as
      a first attempt at identifying stat types that are more independent,
      carrying unique information. It is observed that there are only 16 out of
      45 (35%) of stats that have maximum correlations under 0.8. A correlation
      of 0.8 is still strong and thus not ideal. It was important to explore
      this further to see if the entire row of correlations for a given stat
      were also high (we needed a sense of the distribution of correlations per
      stat). Thus we have the following chart:
    </p>
    <figure class="d-flex flex-column align-items-center">
      <img src="{{ url_for('static', filename='images/machine_learning/percentile_correlation_coefficients.png') }}"
           alt="percentile correlation coefficients"
         style="width: 654px;" />
      <figcaption class="pt-1 text-white">Fig 3. Stats with 80% of their correlation coefficients < 0.35</figcaption>
    </figure>
    <br>
    <p class="text-white w-100">
      Here, we're looking at the 80th percentile correlation value for each stat
      that is filtered to leave only the stats where 80% of their correlations
      were below 0.35. This gives us a sense per stat of how independent it is
      from other stats on average. Starting with the important stats revealed by
      the previous sections, we can now add in stats that on average are
      independent and thus provide unique information to the model. The
      following chart shows the heatmap for the final selection of features
      based on an iterative process of running the machine learning model,
      viewing the feature importance, and deciding to add or subtract stats from
      the feature set:
    </p>
    <figure class="d-flex flex-column align-items-center">
      <img src="{{ url_for('static', filename='images/machine_learning/correlation_heatmap_selected_features.png') }}"
           alt="correlation heatmap for selected features"
         style="width: 654px;" />
      <figcaption class="pt-1 text-white">Fig 4. The final selection of stats to be used as features in our model</figcaption>
    </figure>
    <br>
    <p class="text-white w-100">
      Principal Component Analysis was also completed using the full set of
      stats to investigate the amount of components needed to explain most of
      the variance in the data. This helped us in identifying the number of
      components to shoot for by the end of the feature engineering iterative
      process.
    </p>
    <figure class="d-flex flex-column align-items-center">
      <img src="{{ url_for('static', filename='images/machine_learning/cumulative_explained_variance_all_features.png') }}"
           alt="cumulative explained variance for all features"
         style="width: 654px;" />
      <figcaption class="pt-1 text-white">Fig 5. Cumulative explained variance</figcaption>
    </figure>
    <br>
  </div>
  <br>
  <div class="d-flex flex-column align-items-center">
    <h4 class="align-self-start text-white">Results</h4>
    <p class="text-white w-100">
      After running the model and predicting the test dataset, the following
      confusion matrix shows the performance of our model:
    </p>
    <figure class="d-flex flex-column align-items-center">
      <img src="{{ url_for('static', filename='images/machine_learning/confusion_matrix.png') }}"
           alt="confusion matrix"
         style="width: 654px;" />
      <figcaption class="pt-1 text-white">Fig 6. Model results: confusion matrix</figcaption>
    </figure>
    <br>
    <p class="text-white w-100">
      We can see that we predicted champions correctly at a rate of 50%. This is
      less than ideal and may be due to a lack of data when limiting the input
      data to the year 2016, and only reaching a total 7 features used. Moving
      on to the goal of the project, our model predicted the Memphis Grizzlies
      as having the highest probability of winning the championship in 2022. See
      below the resulting probabilities for all playoff teams.
    </p>
		<div class="container">
			<div class="table-responsive" style="height:300px; width: 700px; overflow-y:scroll">
				{{ proba_est_table_html|safe }}
			</div>
		</div>
    <br>
  </div>
  <br>
  <div class="d-flex flex-column align-items-center">
    <h4 class="align-self-start text-white">Feature Importance</h4>
    <p class="text-white w-100">
      It was important in developing an iterative process to refine our model to
      have some sense of the most important features at the end of each run.
      Feature importance helped us achieve this. We used the coefficients from
      the logistic regression model as our measure and we created a relative
      scale based on the stat with the largest coefficient (the most important
      stat). Below is the visual showcasing the results for our final iteration
      using the final feature set:
    </p>
    <figure class="d-flex flex-column align-items-center">
      <img src="{{ url_for('static', filename='images/machine_learning/feature_importance.png') }}"
           alt="feature importance"
         style="width: 654px;" />
      <figcaption class="pt-1 text-white">Fig 8. Relative feature importance</figcaption>
    </figure>
    <br>
		<div class="container">
			<div class="table-responsive" style="width: 300px;">
				{{ feat_imp_table_html|safe }}
			</div>
		</div>
    <br>
    <p class="text-white w-100">
      We can see here that assist to turnover ratio is the most important to
      predicting our champions. Other stats that proved to be strong
      indicators of champion likely teams were defensive and offensive rating
      as well as blocks and steals.
    </p>
  </div>
  <br>
  <tableau-viz id="tableauViz"
              src="https://public.tableau.com/views/NBATeamStats_16516195895630/MemphisGrizzlies_1?:language=en-US&:display_count=n&:origin=viz_share_link"
           device="desktop"
          toolbar="bottom"
        hide-tabs>
  </tableau-viz>
  <div class="pt-5" id="featured-3">
    <div class="pb-2">
      <h2 class="pb-1 border-bottom text-white">Analysis</h2>
    </div>
    <ul class="text-white">Memphis Grizzlies
      <li>
        Our machine learning model predicted that the Memphis Grizzlies will
        be the 2022 NBA Finals Champions. After we ran our machine learning
        model, we wanted to see the highest weighted relative importance of
        each of the stats that were accounted for in predicting the 2022
        champion. The top four features were Assist to Turnover Ratio (100),
        Defensive Rating (86.02), Offensive Rating (50.12), and Blocks (45.9).
        In the graph above, you can see the seasonal trends over the last few
        decades for these four features. If we look at the other dashboards in
        the story, we can see how the Grizzlies compare to other playoff teams
        in each of the four features.
      </li>
    </ul>
  </div>
</div>
{% endblock body %}
